#!/usr/bin/env python

# hadoop-job-analyzer
#
# This program analyzes the history/done folder of a hadoop jobtracker, aggregates metrics about the 
# jobs and sends them to some monitoring system.
#
# Currently, only sending to graphite is supported. Other monitoring systems might be supported in the future.
#
#
#
# Author: Harel Ben-Attia, harelba on GitHub, @harelba on Twitter
#
# May 2013

import os,sys,time
import math
import socket
import csv
import re
import glob
from optparse import OptionParser
import traceback
import logging
import logging.handlers
import random
import fnmatch
from xml.etree.cElementTree import iterparse
import cPickle

class CachedMetricProxy(object):
	def __init__(self,metric_client,key):
		self.FILENAME = '/tmp/last_metric_tuples_%(key)s.pickle' % vars()
		logging.info("Initializing cached metric proxy using filename %s" % self.FILENAME)
		self.metric_tuple_map = {}
		try:
			self.last_metric_tuple_map = cPickle.load(file(self.FILENAME))
		except:
			logging.warning("Warning! Metric proxy cache file has not been found. Lots of metrics will be sent during this execution. The cache will be created for further executions, so next executions will be more efficient")
			self.last_metric_tuple_map = {}
		self.metric_client = metric_client
		self.send_request_count = 0
		self.actual_metrics_sent = 0

	def initialize(self,params):
		self.metric_client.initialize(params)

	def start_projection(self,proj):
		self.metric_client.start_projection(proj)

	def end_projection(self,proj):
		self.metric_client.end_projection(proj)

	def add_metric(self,name,value,timestamp):
		self.send_request_count += 1
		t = (name,value,timestamp)
		self.metric_tuple_map[t] = t
		if not t in self.last_metric_tuple_map:
			self.actual_metrics_sent += 1
			self.metric_client.add_metric(name,value,timestamp)

	def done(self):
		cPickle.dump(self.metric_tuple_map,file(self.FILENAME,'w'))
		logging.info("Sending only %d metrics out of %d metric requests - All skipped metrics have already been sent",self.actual_metrics_sent,self.send_request_count)
		self.metric_client.done()

class CounterGroup(object):
	"""
	Represents a hadoop counter group, along with its counters
	"""
	def __init__(self,name,desc,counters):
		self.name = name.replace("\\","").replace(".","_")
		self.desc = desc
		self.counters = counters

	def __str__(self):
		return "name=%s,counters=%d" % (self.name,len(self.counters))
	__repr__ = __str__

class Counter(object):
	"""
	Represents one hadoop counter
	"""
	def __init__(self,name,desc,value):
		self.name = name.replace("\\","").replace(".","_")
		self.desc = desc
		self.value = value

	def __str__(self):
		return "%s=%s" % (self.name,self.value)
	__repr__ = __str__

class Counters(object):
	"""
	Parses a runtime job file counters string into relevant counter groups and counters
	"""
	def __init__(self,counters_str):
		self.counters_str = counters_str
		self.counter_groups = []
		self.analyze()

	def analyze(self):
		# extract counter group info
		counter_groups_re = re.compile('{\\((?P<name>.*?)\\)\\((?P<desc>.*?)\\)(?P<counters>.*?)}')
		for counter_group in counter_groups_re.finditer(self.counters_str):
			# extract counter info
			counters_for_group_str = counter_group.group('counters')
			individual_counters_re = re.compile('\\[\\((?P<name>.*?)\\)\\((?P<desc>.*?)\\)\\((?P<value>.*?)\\)\\]')

			counters = []
			for counter in individual_counters_re.finditer(counters_for_group_str):
				c = Counter(counter.group('name'),counter.group('desc'),counter.group('value'))
				counters.append(c)

			cg = CounterGroup(counter_group.group('name'),counter_group.group('desc'),counters)
			self.counter_groups.append(cg)

	def __str__(self):
		return "%d groups" % (len(self.counter_groups))
	__repr__ = __str__

class JobNameMetadataAnalyzer(object):
	def __init__(self,separator):
		self.separator = separator

	def analyze(self,jobname):
		try:
			if '\n' in jobname or '\t' in jobname:
				raise Exception('job names cannot contain new lines or tabs')
			if self.separator not in jobname:
				jobname = 'NON_COMPLIANT_JOB_NAME %s' % jobname.replace(" ","_")
				logging.warning("Found non compliant job name. Changed it to %s" % jobname)
			pairs = jobname.split(self.separator)
			# Part of the trick described below... We translated all = signs to spaces, so we're looking for a space here and not an equal sign
			keyvalues = [tuple(pair.split(" ",1)) for pair in pairs]
			
			return dict(keyvalues)
		except:
			logging.error(traceback.format_exc())
			return None

class Job(object):
	def __init__(self,conf_filename,runtime_filename,time_bucket_field,time_bucket_interval,mapped_job_name_metadata,mapped_job_name_separator,maximum_jobname_metadata_length):
		self.time_bucket_field = time_bucket_field
		self.time_bucket_interval = time_bucket_interval
		self.c = JobConfFile(conf_filename)
		# TODO blacklisted counter pattern should be exposed as a parameter. Currently it removes all MultiInput/MultipleOutputs counters, which 
		# contain a lot of per-execution garbage
		self.r = JobRuntimeFile(runtime_filename,mapped_job_name_metadata,mapped_job_name_separator,maximum_jobname_metadata_length,'(MultiInput|MultipleOutputs)')
		self.data = {}
		self.analyze()

	def analyze(self):
		self.data.update(self.r.data)
		self.data.update(self.c.data)

		self.numerify_metrics()
		self.convert_times_to_seconds()
		self._calculate_durations_from_data()
		self._create_time_bucket()

	def convert_times_to_seconds(self):
		"""
		Converts all time related metrics to seconds instead of ms
		"""
		for k in self.data.keys():
			if '_TIME' in k:
				self.data[k] = float(self.data[k]/1000.0)

	def numerify_metrics(self):
		""" 
		Converts all string numbers into actual floats
		"""
		for k in self.data.keys():
			try:
				float(self.data[k])
				self.data[k] = float(self.data[k])
			except:
				pass

	def _calculate_durations_from_data(self):
		# Calculate durations based on the time data
		self.data['LAUNCH_LATENCY'] = self.data['LAUNCH_TIME'] - self.data['SUBMIT_TIME']
		self.data['TOTAL_DURATION'] = self.data['FINISH_TIME']- self.data['SUBMIT_TIME']
		self.data['ACTUAL_DURATION'] = self.data['FINISH_TIME'] - self.data['LAUNCH_TIME']

	def _create_time_bucket(self):
		"""
		TIME_BUCKET is the data being used for grouping the metrics by time.
		
		It is determined by rounding the provided time_bucket_field field.

		For example, if time_bucket_field is 'SUBMIT_TIME' then all jobs will be aggregated over time using the rounded submit time
		"""
		# Create the time bucket to which the job should belong to.
		self.data['TIME_BUCKET'] = int(math.floor(int(self.data[self.time_bucket_field]/float(self.time_bucket_interval))*float(self.time_bucket_interval)))


class JobRuntimeFile(object):
	"""
	Encapsulates the parsing and data of a hadoop job runtime file
	"""
	def __init__(self,job_runtime_filename,mapped_job_name_metadata,mapped_job_name_seperator,maximum_jobname_metadata_length,blacklisted_counter_name_pattern_str):
		self.job_runtime_filename = job_runtime_filename
		self.mapped_job_name_metadata = mapped_job_name_metadata
		self.mapped_job_name_separator = mapped_job_name_separator
		self.maximum_jobname_metadata_length = maximum_jobname_metadata_length
		self.data = {}
		self.job_name_parsing_error = False
		self.blacklisted_counter_name_pattern = re.compile(blacklisted_counter_name_pattern_str)
		self.read_job_runtime_filename()
		self.analyze_counters()
		self.flatten_counters()
		self.enrich_data()

	def should_ignore_counter(self,k):
		m = self.blacklisted_counter_name_pattern.search(k)
		return m is not None		
				
	def flatten_counters(self):
		"""
		Flattens all counter instances into separate counter_group.counter.value
		"""
		self.new_data = {}
		for k,v in self.data.iteritems():
			if not 'COUNTERS' in k:
				self.new_data[k] = v
			else:
				self.new_entries = {}
				for counter_group in v.counter_groups:
					for counter in counter_group.counters:
						if self.should_ignore_counter(counter_group.name) or self.should_ignore_counter(counter.name):
							logging.info("Skipping counter that matches blacklisted counter name pattern. counter group name is %s counter name is %s" % (counter_group.name,counter.name))
							continue
						self.new_entries['%s.%s.%s' % (k,counter_group.name,counter.name)] = counter.value
				self.new_data.update(self.new_entries)

		self.data = self.new_data
						
	def analyze_counters(self):
		"""
		Converts all counter information to an actual Counters object
		"""
		counters = {}
		for k,v in self.data.iteritems():
			if 'COUNTERS' in k:
				c = Counters(v)
				counters[k] = c

		self.data.update(counters)
				
	def _enrich_data_using_job_name(self):
		if not self.mapped_job_name_metadata:
			return

		job_name_analyzer = JobNameMetadataAnalyzer(self.mapped_job_name_separator)

		job_name_metadata_map = job_name_analyzer.analyze(self.data['JOBNAME'])
		self.job_name_parsing_error = job_name_metadata_map is None

		if job_name_metadata_map is None:
			return

		for k,v in job_name_metadata_map.iteritems():
			if self.maximum_jobname_metadata_length is None or len(v) <= self.maximum_jobname_metadata_length:
				self.data[k] = v
			else:
				self.data[k] = 'toolong'

	def _flatten_job_status(self):
		# Flatten job status
		job_status = self.data['JOB_STATUS']
		self.data['JOB_STATUS.%s' % job_status] = 1
		del self.data['JOB_STATUS']

	def enrich_data(self):
		"""
		Adds derived data, based on existing data

		"""
		self._enrich_data_using_job_name()
		self._flatten_job_status()

	def read_job_runtime_filename(self):
		"""
		Read the hadoop job runtime file. 
		NOTE: In order to support multi-line fields, we use a trick here - We write 
		      the file to a temp file, translating = chars to spaces, and then read
		      the file as a space delimited csv.
		TODO: Make this part multi-thread safe, so we can make things parallel in the future
		"""

		if os.path.getsize(self.job_runtime_filename) == 0:
			raise Exception('Job Runtime file is of size 0 - no use in trying to analyze it')

		f = file(self.job_runtime_filename,'rb')
                temp_file = file('/var/tmp/job-runtime-file.tmp',"wb")
                while 1:
                        buf = f.read(16384)
			buf = buf.replace("="," ")
                        if not buf:
                                break
                        temp_file.write(buf)
                temp_file.close()
                f.close()

                f = file('/var/tmp/job-runtime-file.tmp',"rb")

		self.data = {}
		reader = csv.reader(f,delimiter=' ',quotechar='"',escapechar='\\')
		for line in reader:
			if len(line) == 0:
				continue
			if line[0] == 'Job':
				line = line[1:]
				m = dict(zip(line[0::2],line[1::2]))
				self.data.update(m)

	def __str__(self):
		return str(self.data)
	__repr__ = __str__
		
		
class JobConfFilename(object):
	def __init__(self,job_conf_filename):
		self.job_conf_filename = job_conf_filename
		self.jt = None
		self.jt_start_ts = None
		self.job_ts = None
		self.job_number = None
		self.analyze_filename()

	def analyze_filename(self):
		base_name = os.path.split(self.job_conf_filename)[1]
		try:
			self.jt,self.jt_start_ts,_,self.job_ts,self.job_number,_ = base_name.split("_",5)
			self.job_id = "job_%s_%s" % (self.job_ts,self.job_number)
		except:
			try:
				_,self.job_ts,self.job_number,_ = base_name.split("_",3)
				self.job_id = "job_%s_%s" % (self.job_ts,self.job_number)
			except:
				raise Exception("Conf file name format is undetectable")
		
class JobConfFile(object):
	"""
	Encapsulates the parsing and data of a job conf file
	"""
	def __init__(self,job_conf_filename):
		self.job_conf_filename = job_conf_filename
		self.jt = None
		self.jt_start_ts = None
		self.job_ts = None
		self.job_number = None
		self.properties = {}
		self.data = {}
		self.analyze()

	def analyze(self):
		self.data = {}

		# Add filename information as fields
		jcf = JobConfFilename(self.job_conf_filename)
		self.data['JOB_ID'] = jcf.job_id
		self.data['JOB_TRACKER_START_TIME'] = jcf.jt_start_ts
		self.data['JOB_NUMBER'] = jcf.job_number

		if os.path.getsize(self.job_conf_filename) == 0:
			raise Exception('Conf file is of size 0 - no use in analyzing. file name = %s' % self.job_conf_filename)

		self.properties = {}
		name = None
		value = None
		for event,elem in iterparse(self.job_conf_filename):
			if elem.tag == 'name':
				name = elem.text
			elif elem.tag == 'value':
				if name == None:
					logging.error("Could not find matching name/value when reading conf file %s. Trying to continue" % self.job_conf_filename)
					continue
				value = elem.text
				if name is not None and value is not None:
					self.properties[name] = value
				name = None
				value = None

		if 'mapreduce.job.submithost' in self.properties:
			self.data['SOURCE_HOST'] = self.properties['mapreduce.job.submithost'].replace(".","_")
		else:
			self.data['SOURCE_HOST'] = 'unknown-host'

	def __str__(self):
		return "job_conf_filename=%s,data=%s" % (self.job_conf_filename,self.data)
	__repr__ = __str__


class JobHistoryProvider(object):
	"""
	A provider for the job history files. Iterate over job_history() in order to get conf,runtime job filename pairs

	"""
	def __init__(self,history_folder,relaxed=True,conf_filename_postfix='_conf.xml'):
		self.history_folder = history_folder
		self.conf_filename_postfix = conf_filename_postfix
		self.pairs = []
		self.relaxed = relaxed
		self.job_error_count = 0
		self.analyze()

	def analyze(self):
		conf_filenames = []
		runtime_filenames = []
		p = re.compile('(?P<job_id>job_[0-9]+_[0-9]+)_')
		runtime_filenames_map = {}
		# Collect all conf/runtime files from the disk recursively (CDH4 has the data partitioned in multiple folders)
		for root,dirnames,filenames in os.walk(self.history_folder):
			for filename in filenames:
				if fnmatch.fnmatch(filename,'*%s' % self.conf_filename_postfix):
					conf_filenames.append('%s/%s' % (root,filename))
				elif not fnmatch.fnmatch(filename,'*.crc'):
					runtime_filenames.append('%s/%s' % (root,filename))
					m = p.search(filename)
					if m is not None:
						runtime_filenames_map[m.group('job_id')] = '%s/%s' % (root,filename)
		i = 0
		for conf_filename in conf_filenames:
			if i % 100 == 0:
				logging.info("Passed %s conf filenames" % i)
			i += 1
			jcf = JobConfFilename(conf_filename)
			job_id = jcf.job_id
			if job_id in runtime_filenames_map:
				self.pairs.append((conf_filename,runtime_filenames_map[job_id]))
			else:
				logging.error('Could not find matching runtime file for conf file %s' % conf_filename)
				self.job_error_count += 1
				if not relaxed:
					raise Exception('Could not find matching runtime file for conf file %s' % conf_filename)
				else:
					logging.warning("Relaxed mode - skipping file %s" % conf_filename)
		
	def job_history_length(self):
		return len(self.pairs)

	def job_history(self):
		for pair in self.pairs:
			yield pair

class Projection(object):
	"""
	A projection is one specific "group by" of raw data, in this case job data.

	Each projection is a map: { time_bucket -> { projection_key -> { metric -> value } } }

	time_bucket_field signifies the field name which includes the time_bucket value
	spec is a tuple with the names of fields to project by (e.g. the keys of projection_keys)
	"""
	def __init__(self,spec,relaxed=True):
		self.spec = spec
		self.projection = {}
		self.relaxed = relaxed

	def __str__(self):
		return "<spec=%s>" % str(self.spec)
	__repr__ = __str__

	def generate_projection_key(self,spec,data):
		"""
		Generate the projection key for data
		"""
		k = []
		for field in spec:
			if field in data.keys():
				k.append(data[field])
			else:
				k.append('value-unknown')
		return tuple(k)

	def remove_unneeded_values(self,data):
		"""
		Removes all non-value jobs and all time values from data.

		composite values should have been flattened by now, and time values are not really worth aggregating.
		enrich_data should have converted absolute times to durations at that point
		"""
		self.new_data = {}
		for k,v in data.iteritems():
			try:
				# Make sure it's a number
				tmp = float(v)
				# Remove all absolute time data
				if '_TIME' not in k and k != 'TIME_BUCKET':
					self.new_data[k] = v
			except:
				pass
		return self.new_data

	def merge_values(self,time_bucket,projection_key,clean_data):
		"""
		Aggregates new data with existing data. Currently only summation is supported

		TODO: Possibly support other types of aggregations. Not sure it's needed, though
		"""	
		m = self.projection[time_bucket][projection_key]
		for k,v in clean_data.iteritems():
			if not k in m:
				m[k] = v
			else:
				# Only sum is supported for now
				m[k] = m[k] + v
		
	def add_data(self,data):
		"""
		Add new data to the projection
		"""
		try:
			time_bucket = data['TIME_BUCKET']
			projection_key = self.generate_projection_key(self.spec,data)
		except Exception,e:
			logging.error("Could not generate time bucket and projection key. spec=%s" % str(self.spec))
			if self.relaxed:
				logging.warning("Non strict mode. Skipping job %s" % data['JOB_ID'])
				return
			else:
				raise e
				
		# if it's a new time bucket
		if time_bucket not in self.projection:
			# Create the bucket
			self.projection[time_bucket] = {}
		# If it's a new projection key
		if projection_key not in self.projection[time_bucket]:
			# Create the projection key
			self.projection[time_bucket][projection_key] = {}

		# Clean the data
		clean_data = self.remove_unneeded_values(data)

		# and merge the new data with existing values
		self.merge_values(time_bucket,projection_key,clean_data)

	def normalize(self,metric):
		"""
		Remove special characters from the metric name
		"""
		if metric is None or str(metric).strip() == '':
			return "empty-value"
		else:
			return str(metric).strip().replace("$","__").replace(",","_").replace(" ","_")

	def fetch_projection_values(self,prefix):
		"""
		Returns a list of tuples (metric_name,metric_value,metric_time_bucket), which contain 
		the values of the projection.

		prefix is prepended to all metric names
		"""
		projection_spec_str = self.normalize("-".join(self.spec))
		projection_nodes_str = self.normalize(".".join(self.spec))
		values = []
		for time_bucket,v1 in self.projection.iteritems():
			for projection_key,v2 in v1.iteritems():
				projection_nodes_str = ".".join([self.normalize(x) for x in projection_key])
				for metric,value in v2.iteritems():
					metric = self.normalize(metric)
					metric_name = '%(prefix)s%(projection_spec_str)s.%(projection_nodes_str)s.%(metric)s.value' % vars()
					metric_value = value
					metric_time_bucket = time_bucket
					values.append((metric_name,metric_value,metric_time_bucket))

		return values

class HadoopJobParser(object):
	"""
	Parses job files in order to provide job metadata
	"""
	def __init__(self,history_folder,time_bucket_field,time_bucket_interval,mapped_job_name_metadata,mapped_job_name_separator,maximum_jobname_metadata_length,relaxed):
		self.history_folder = history_folder
		self.time_bucket_field = time_bucket_field
		self.time_bucket_interval = time_bucket_interval
		self.mapped_job_name_metadata = mapped_job_name_metadata
		self.mapped_job_name_separator = mapped_job_name_separator
		self.maximum_jobname_metadata_length = maximum_jobname_metadata_length
		self.relaxed = relaxed
		self.jobs = []

		self.job_parsing_error_count = 0
		self.job_history_error_count = 0
		self.job_name_parsing_error_count = 0

	def parse(self):
		self.jobs = []
		jhp = JobHistoryProvider(self.history_folder,self.relaxed)
		self.job_history_error_count = jhp.job_error_count
		for i,jh in enumerate(jhp.job_history()):
			conf_filename,runtime_filename = jh
			try:
				if i % 10 == 0:
					logging.info("Handling file %d out of %d " % (i,jhp.job_history_length()))
				j = Job(conf_filename,runtime_filename,self.time_bucket_field,self.time_bucket_interval,self.mapped_job_name_metadata,self.mapped_job_name_separator,self.maximum_jobname_metadata_length)
				if j.r.job_name_parsing_error:
					self.job_name_parsing_error_count += 1
	
				self.jobs.append(j)
			except Exception,e:
				self.job_parsing_error_count += 1
				if self.relaxed:
					logging.error("Exception while handling files %s,%s. Relaxed mode, continuing to next job file. %s" % (conf_filename,runtime_filename,traceback.format_exc()))
				else:
					logging.error("Exception while handlnig files %s,%s. Stopping. Consider relaxed mode if needed. %s" % (conf_filename,runtime_filename,traceback.format_exc()))
					raise
		logging.info("Done handling files")

	def get_available_fields(self,sample_count=3):
		# get all available fields
		field_names = set([])
		for job in self.jobs:
			field_names = field_names.union(job.data.keys())

		m = {}
		for field_name in field_names:
			l = range(len(self.jobs))
			random.shuffle(l)
			sample_positions = l[:sample_count]
			samples = []
			for pos in sample_positions:
				sample_data = self.jobs[pos].data
				if not field_name in sample_data.keys():
					samples.append('<non-existent>')
				else:
					samples.append(sample_data[field_name])

			m[field_name] = samples

		return m
		
	
class HadoopJobDataProjector(object):
	def __init__(self,jobs,projection_specs,relaxed):
		self.jobs = jobs
		self.relaxed = relaxed
		self.projection_specs = projection_specs

	def project(self):
		self.projections = []
		for spec in self.projection_specs:
			logging.info("Creating projection %d spec=%s" % (len(self.projections),spec))
			p = Projection(spec,self.relaxed)
			self.projections.append(p)
		
		for i,job in enumerate(self.jobs):
			logging.info("Handling file %d for all projections" % i)
			for proj in self.projections:
				proj.add_data(job.data)

def materialize_metric_name_prefix(metric_name_prefix):
	"""
	Convert prefix template to actual prefix.

	TODO: More general approach. Currently only handling reverse hostnames
	"""
	os_hostname = os.uname()[1]
	if '.' in os_hostname:
		name,domain = os_hostname.split(".",1)
		domain = domain.replace(".","_")
		HOSTNAME = "%s.%s" % (domain,name)
	else:
		HOSTNAME = 'unknown-domain.%s' % os_hostname

	if metric_name_prefix is not None:
		result = metric_name_prefix.replace("${HOSTNAME}",HOSTNAME)
	
		if len(result) > 0 and result[-1] != '.':
			result = result + "."

		return result
	else:
		return 'hja.metrics'

class HadoopJobMetricsSender(object):
	"""
	Sends projects as metrics to some metrics-client
	"""
	def __init__(self,projections,metric_client,metric_name_prefix):
		self.projections = projections
		self.metric_client = metric_client
		self.metric_name_prefix = metric_name_prefix

	def send(self):
		self._send_using_metric_client()

	def _send_using_metric_client(self):
		# For each projection
		for proj in self.projections:
			self.metric_client.start_projection(proj)
			# Fetch all the metrics for that projection
			metrics = proj.fetch_projection_values('%sprojections.' % self.metric_name_prefix)

			logging.info("Requesting send of %d metrics for projection %s (actual metrics sent will be lower, depending on previous data sent to the metric backend)" % (len(metrics),proj))
			for metric in metrics:
				self.metric_client.add_metric(metric[0],'%4.3f' % metric[1],int(metric[2]))
			logging.info("Done Requesting sending %d metrics for projection %s" % (len(metrics),proj))

			self.metric_client.end_projection(proj)

def setup_logging():
	FORMAT = '%(asctime)-15s %(levelname)s %(message)s'

	log_folder = os.path.join(os.path.split(sys.argv[0])[0],'logs')
	if not os.path.exists(log_folder):
		os.makedirs(log_folder)
	log_filename = os.path.join(os.path.split(sys.argv[0])[0],'logs','hadoop-job-analyzer.log')
	logger = logging.getLogger()
	logger.setLevel(logging.INFO)
	handler = logging.handlers.RotatingFileHandler(log_filename,maxBytes=20000000,backupCount=5)
	handler.setFormatter(logging.Formatter(FORMAT))
	logger.addHandler(handler)

def show_error_and_exit(msg,error_code=1,show_help=True):
	logging.error(msg)
	print >>sys.stderr,msg
	if show_help:
		parser.print_help()
	sys.exit(error_code)

def list_available_fields(hadoop_job_parser,sample_count):
	field_data = hadoop_job_parser.get_available_fields(sample_count)
	for k,v in field_data.iteritems():
		if len(v) == 0:
			print k
		else:
			print k
			for sample in v:
				print "  --> %s" % sample

if __name__ == '__main__':
	setup_logging()

	parser = OptionParser()
	parser.add_option("-f","--history-folder",dest="history_folder",default=None,
	                help="Location of jobtracker history/done folder")
	parser.add_option("-p","--projection-specs",dest="projection_specs",default='USER',
	                help="List of required projection specs. Each spec is in the format <FIELD1>.<FIELD2>... Specs are separated by commas")
	parser.add_option("-t","--time-bucket-field",dest="time_bucket_field",default='SUBMIT_TIME',
	                help="Name of field which will be used to associate a job with a time bucket and to perform aggregation over time. The default should rarely be changed")
	parser.add_option("-i","--time-bucket-interval",dest="time_bucket_interval",default="60",
	                help="Required time interval for aggregation")

	parser.add_option("-M","--mapped-job-name-metadata",dest="mapped_job_name_metadata",default=False,action="store_true",
	                help="If set, then job name will be treated as a map of key=value, separated by a separator that is set by -S. if -M is not set, job name will be ignored")
	parser.add_option("-S","--mapped-job-name-separator",dest="mapped_job_name_separator",default="||",
	                help="When -M is used, this parameter can be used to set the string that separates between key-value pairs. Default is ||")

	parser.add_option("-m","--maximum-job-name-metadata-length",dest="maximum_jobname_metadata_length",default=50,
	                help="Maximum length of extracted job metadata to use - If a named group from -j is above this threshold, it will be written as 'toolong' instead of its actual value. This is a safety mechanism against cluttering the metrics for non compliant job names")
	parser.add_option("-n","--metric-name-prefix",dest="metric_name_prefix",default=None,
	                help="Prefix for all metrics. You can add ${HOSTNAME} in the prefix and it will be replaced with 'domain.hostname'. E.g. data.hadoop.jobtracker.${HOSTNAME} . Note that you'd need to use single quotes in the command line so the $ sign will not be parsed by the shell")
	parser.add_option("-C","--metric-client-type",dest="metric_client_type",default=None,
	                help="Type of metric client. currently supported types are 'stdout' and 'graphite'. The 'graphite' type requires the following client params (-P): 'host=X,port=Y'")
	parser.add_option("-P","--metric-client-params",dest="metric_client_params",default=None,
	                help="Comma separated list of parameters in the format x=y. Will be passed to the metric client")
	parser.add_option("-r","--relaxed",dest="relaxed",default=False,action="store_true",
	                help="Relaxed mode. If set, missing fields in a job would cause the system to continue analyzing the next ones")
	parser.add_option("-l","--list-available-fields",dest="should_list_available_fields",default=False,action="store_true",
	                help="Using this flag will provide all the field names available by scanning the currently existing jobs")
	parser.add_option("-s","--sample-count",dest="sample_count",default=3,
	                help="Number of samples to provide when using the -l list flag")
	parser.add_option("-c","--cache-metrics",dest="cache_metrics",default=False,action="store_true",
	                help="Cache metrics locally so identical metrics will not be sent twice to the metric backend. This can elleviate the load of the metric backend in case of a large set of metrics and when the tool is run periodically")


	(options,args) = parser.parse_args()

	if options.history_folder is None:
		show_error_and_exit("History folder must be provided")
		
	if not os.path.exists(options.history_folder):
		show_error_and_exit("History folder %s does not exist" % options.history_folder)

	history_folder = options.history_folder

	should_list_available_fields = options.should_list_available_fields
	sample_count = int(options.sample_count)

	if options.projection_specs is None and not should_list_available_fields:
		show_error_and_exit("At least one projection spec needs to be provided")

	projection_specs = [tuple(spec.split("/")) for spec in options.projection_specs.split(",")]
	time_bucket_field = options.time_bucket_field

	mapped_job_name_metadata = options.mapped_job_name_metadata
	mapped_job_name_separator = options.mapped_job_name_separator

	maximum_jobname_metadata_length = options.maximum_jobname_metadata_length

	if options.metric_name_prefix is None and not should_list_available_fields:
		show_error_and_exit("Metric name prefix needs to be provided")

	metric_name_prefix = materialize_metric_name_prefix(options.metric_name_prefix)

	if options.metric_client_type is None and not should_list_available_fields:
		show_error_and_exit("Metric client type must be provided")

	metric_client_module_name = 'hja-%s' % options.metric_client_type

	if options.metric_client_params is not None:
		metric_client_params = dict([p.split("=",1) for p in options.metric_client_params.split(",")])
	else:
		metric_client_params = {}

	try:
		time_bucket_interval = float(options.time_bucket_interval)
	except:
		show_error_and_exit("time bucket interval must be a number")

	code_path = os.path.split(sys.argv[0])[0]
	if not should_list_available_fields:
		if not os.path.exists('%s/%s.py' % (code_path,metric_client_module_name)):
			show_error_and_exit("Unrecognized metric client type %s. cannot find %s" % (options.metric_client_type,metric_client_module_name))
		else:
			try:
				sys.path.append(code_path)
				key = os.path.abspath(options.history_folder).replace("/","_") + "__" + options.metric_client_type + "__interval" + str(time_bucket_interval)
				metric_client = __import__(metric_client_module_name)
				if options.cache_metrics:
					logging.info("Going to cache metric data")
					metric_client = CachedMetricProxy(metric_client,key)
				else:
					logging.info("Not going to cache metric data")
				logging.info("Using metric client %s" % options.metric_client_type)
				metric_client.initialize(metric_client_params)
				logging.info("Metric client initialized with params %s" % str(metric_client_params))
			except:
				show_error_and_exit("Could not load metric client %s. module name %s not found. Traceback %s" % (options.metric_client_type,metric_client_module_name,traceback.format_exc()))

	relaxed = options.relaxed

	# Main logic starts here
	try:
		start_time = time.time()

		# Parse the job data
		logging.info("Parsing job files")
		parser = HadoopJobParser(history_folder,time_bucket_field,time_bucket_interval,mapped_job_name_metadata,mapped_job_name_separator,maximum_jobname_metadata_length,relaxed)
		parser.parse()

		# If list has been requested, just spit it out and exit
		if should_list_available_fields:
			list_available_fields(parser,sample_count)
		else:
			# Perform aggregations
			logging.info("Performing Aggregations")
			projector = HadoopJobDataProjector(parser.jobs,projection_specs,relaxed)
			projector.project()

			# Send as metrics
			logging.info("Sending metrics")
			metrics_sender = HadoopJobMetricsSender(projector.projections,metric_client,metric_name_prefix)
			metrics_sender.send()

			total_time = time.time() - start_time
			metric_client.add_metric('%sstats.total_time' % metric_name_prefix,total_time,start_time)
			metric_client.add_metric('%sstats.job_parsing_error_count' % metric_name_prefix,parser.job_parsing_error_count,start_time)
			metric_client.add_metric('%sstats.job_history_error_count' % metric_name_prefix,parser.job_history_error_count,start_time)
			metric_client.add_metric('%sstats.job_name_parsing_error_count' % metric_name_prefix,parser.job_name_parsing_error_count,start_time)

			metric_client.done()

			logging.info("Done. Total time taken is %4.3f seconds, with %d job parsing errors, %s job history errors, and %s job name parsing errors" % (total_time,parser.job_parsing_error_count,parser.job_history_error_count,parser.job_name_parsing_error_count))
	except Exception,e:
		logging.fatal("FAILED. Total time taken is %4.3f seconds" % (time.time() - start_time))
		show_error_and_exit("There was an error during job analysis. Consider using relaxed flag if needed. %s" % traceback.format_exc(),error_code=100,show_help=False)




